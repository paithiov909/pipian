---
title: "Practice: Dependency Parsing from NLP 100 Exercise 2020 (Rev1)"
author: "Kato Akiru"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
header-includes:
  - \usepackage[utf8]{inputenc}
vignette: >
  %\VignetteIndexEntry{Practice: Dependency Parsing from NLP 100 Exercise 2020 (Rev1)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)

stopifnot(require(pipian))
stopifnot(require(tidyverse))
stopifnot(require(readtext))
```

## Overview

### この記事について

[言語処理100本ノック 2020 (Rev 1) ](https://nlp100.github.io/ja/)のうち、[第5章: 係り受け解析](https://nlp100.github.io/ja/ch05.html)の回答例。

- Rev 1なので初版とは一部の内容が異なる（初版では夏目漱石『吾輩は猫である』が用いられていた）。
- NLP100ノックのこの章では、Pythonなどで解かれることを念頭に`Morph`クラスを実装してそれを拡張しながら解き進めていくつくりになっている。一方で、この記事では[Tidyverse](https://www.tidyverse.org/)的なデータフレームの操作に注力し、クラスを実装せずに回答をつくっている。

### 言語処理100本ノック 2020について

[2015年版](http://www.cl.ecei.tohoku.ac.jp/nlp100/)については以下の例がRで取り組んでいる。

- [Rによる言語処理100本ノック前半まとめ - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/07/27/001728)
- [Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/10/22/193839)

2020年版ではPythonで解いている記事はすでにたくさんある。Rで部分的に取り組んでいる例もあるが、とくに8章のディープ・ニューラルネットのあたりから現状のRを取り巻く環境では取り組みにくい課題になるためか、完走している例はまだ見つけられない。

- [【R】言語処理100本ノック - Qiita](https://qiita.com/PiyoMoasa/items/7c1a6cca3f9cbcaf7773)
- [R言語で「言語処理100本ノック 2020」/ NLP100 Rlang - Speaker Deck](https://speakerdeck.com/upura/nlp100-rlang)

### 使用するファイルの読み込み

83文ある（各文は複文なので本当はパラグラフというほうが正しい）。

```{r read_file}
temp <- tempfile(fileext = ".zip")
download.file("https://nlp100.github.io/data/ai.ja.zip", temp)
temp <- unzip(temp, exdir = tempdir())
ai_ja <- readtext::readtext(temp[1], encoding = "UTF-8")
ai_ja$text[1] %>%
  readr::read_lines(skip_empty_rows = TRUE) %>%
  length()
```

## 回答集

### 40. 係り受け解析結果の読み込み（形態素）

`base::iconv()`は変換先にない文字が含まれると`NA_character_`が返ってくるが、これをそのままにして解析すると`pipian::CabochaR()$as_tibble()`したときに結果をパースできなくなるため、CP932にない文字列が含まれる文はあらかじめ除去する必要がある。

なお、vignetteのビルドに時間がかかりすぎると開発の都合上不便なため、以下では20文だけ抽出して解析している。

```{r knock_40_1}
res <- ai_ja %>%
  dplyr::pull(text) %>%
  readr::read_lines(skip_empty_rows = TRUE)
res <- res %>%
  sample(20L) %>%
  iconv(from = "UTF-8", to = "CP932") %>%
  purrr::discard(~ is.na(.)) %>%
  pipian::cabochaFlatXML()
    
res <- pipian::CabochaR(res)$as_tibble()
head(res)
```

冒頭の説明文の形態素列

```{r knock_40_2}
res %>%
  dplyr::filter(sentence_idx == 1) %>%
  dplyr::pull(word) %>%
  stringr::str_c(collapse = " ")
```

### 41. 係り受け解析結果の読み込み（文節・係り受け）

省略

### 42. 係り元と係り先の文節の表示

```{r knock_42}
memo <- res %>%
  dplyr::filter(POS1 != "記号") %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
    )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk) %>%
  dplyr::distinct()

memo %>%
  dplyr::filter(D2 != -1) %>%
  dplyr::group_by(sentence_idx, chunk_idx, D1) %>%
  dplyr::mutate(collocation = stringr::str_c(
    chunk,
    memo$chunk[memo$sentence_idx == .data$sentence_idx & memo$D1 == .data$D2],
    sep = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(chunk, collocation) %>%
  head()
```

### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出

```{r knock_43}
memo <- res %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
    )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(tag = POS1 == "動詞") %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk, POS1, tag) %>%
  dplyr::distinct()

memo %>%
  dplyr::filter(POS1 == "名詞") %>%
  dplyr::filter(D2 != -1) %>%
  dplyr::group_by(sentence_idx, chunk_idx, D1) %>%
  dplyr::mutate(collocation = stringr::str_c(
    chunk,
    memo$chunk[memo$sentence_idx == .data$sentence_idx & memo$D1 == .data$D2 & memo$tag == TRUE],
    sep = " "
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(chunk, collocation) %>%
  dplyr::filter(chunk != collocation) %>%
  head()
```

### 44. 係り受け木の可視化

```{r knock_44}
graph <- ai_ja$text[1] %>%
  readr::read_lines(skip_empty_rows = TRUE)
tbl <- graph[12] %>%
  iconv(from = "UTF-8", to = "CP932") %>%
  purrr::discard(~ is.na(.)) %>%
  pipian::CabochaTbl()
tbl$plot()
```

### 45. 動詞の格パターンの抽出

```{r knock_45_1}
memo <- res %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original)

pattern <- memo %>%
  dplyr::filter(POS1 == "動詞") %>%
  dplyr::group_by(sentence_idx, chunk_idx, D1) %>%
  dplyr::group_map(~ .x %>%
    dplyr::mutate(collocation = stringr::str_c(
      "",
      memo$Original[memo$sentence_idx == .y$sentence_idx & memo$D2 == .y$D1 &  memo$POS1 == "助詞"],
      collapse = " "
  ))) %>%
  purrr::map_dfr(~ .) %>%
  dplyr::select(Original, collocation)

pattern
```

「行う」「なる」「与える」という動詞の格パターン

```{r knock_45_2}
pattern %>%
  dplyr::filter(Original %in% c("行う", "なる", "与える")) %>%
  dplyr::group_by(Original, collocation) %>%
  dplyr::count()
```

### 46. 動詞の格フレーム情報の抽出

```{r knock_46}
memo <- res %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk)

pattern <- memo %>%
  dplyr::filter(POS1 == "動詞") %>%
  dplyr::group_by(sentence_idx, chunk_idx, D1) %>%
  dplyr::group_map(~ .x %>%
    dplyr::mutate(collocation = stringr::str_c(
      "",
      memo$Original[memo$sentence_idx == .y$sentence_idx & memo$D2 == .y$D1 &  memo$POS1 == "助詞"],
      collapse = " "
    )) %>%
    dplyr::mutate(chunk = stringr::str_c(
      "",
      memo$chunk[memo$sentence_idx == .y$sentence_idx & memo$D2 == .y$D1 &  memo$POS1 == "助詞"],
      collapse = " "
    ))
  ) %>%
  purrr::map_dfr(~ .) %>%
  dplyr::select(Original, collocation, chunk)

pattern
```

### 47. 機能動詞構文のマイニング

「サ変接続名詞 + を -> 動詞」ではなく、サ変接続名詞が含まれる文節すべてについてマイニングしている。

```{r knock_47}
memo <- res %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, POS2, Original, chunk)

pattern <- memo %>%
  dplyr::filter(POS2 == "サ変接続") %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(surface_form = stringr::str_c(
    chunk,
    collapse = ""
  )) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(sentence_idx, chunk_idx, D1) %>%
  dplyr::group_map(~ .x %>%
    dplyr::mutate(collocation = stringr::str_c(
      "",
      memo$Original[memo$sentence_idx == .y$sentence_idx & memo$D2 == .y$D1 &  memo$POS1 == "助詞"],
      collapse = " "
    )) %>%
    dplyr::mutate(chunk = stringr::str_c(
      "",
      memo$chunk[memo$sentence_idx == .y$sentence_idx & memo$D2 == .y$D1 &  memo$POS1 == "助詞"],
      collapse = " "
    ))
  ) %>%
  purrr::map_dfr(~ .) %>%
  dplyr::select(surface_form, collocation, chunk)

pattern
```

### 48. 名詞から根へのパスの抽出

```{r knock_48}
memo <- res %>%
  dplyr::mutate_at(c("D1", "D2"), as.integer) %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::mutate(
    chunk = stringr::str_c(
      word,
      collapse = ""
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) %>%
  dplyr::distinct(chunk, .keep_all = TRUE)

pattern <- memo %>%
  dplyr::filter(POS1 == "名詞") %>%
  dplyr::group_by(sentence_idx, chunk_idx) %>%
  dplyr::group_map(~ .x %>%
    dplyr::mutate(
      sentence_idx = .y$sentence_idx,
      chunk_idx = .y$chunk_idx,
      path = stringr::str_c(
        memo$chunk[
          memo$sentence_idx == .y$sentence_idx &
          memo$chunk_idx >= .y$chunk_idx &
          (memo$D2 > .x$D1 || memo$D2 ==  -1L)
        ],
        sep = "",
        collapse = " -> "
      )
    )
  ) %>%
  purrr::map_dfr(~ .)

pattern %>%
  dplyr::select(path) %>%
  dplyr::filter(path != "")
```

### 49. 名詞間の係り受けパスの抽出

省略

## セッション情報

```{r}
sessioninfo::session_info()
```

